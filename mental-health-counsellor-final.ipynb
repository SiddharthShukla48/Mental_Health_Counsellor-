{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q datasets","metadata":{"execution":{"iopub.status.busy":"2024-04-16T16:53:07.368750Z","iopub.execute_input":"2024-04-16T16:53:07.369155Z","iopub.status.idle":"2024-04-16T16:53:21.916720Z","shell.execute_reply.started":"2024-04-16T16:53:07.369128Z","shell.execute_reply":"2024-04-16T16:53:21.915537Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-04-16T16:53:21.918665Z","iopub.execute_input":"2024-04-16T16:53:21.918971Z","iopub.status.idle":"2024-04-16T16:53:22.229358Z","shell.execute_reply.started":"2024-04-16T16:53:21.918944Z","shell.execute_reply":"2024-04-16T16:53:22.228657Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c76bb57e1c3a47b5b3ecd3b847e5de06"}},"metadata":{}}]},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load the second dataset\nsecond_dataset = load_dataset('Amod/mental_health_counseling_conversations')\n\n# Select only 1000 rows from the dataset\n#second_dataset_subset = second_dataset['train'].shuffle(seed=42).select(range(1000))\n\n# Define a function to transform the data\ndef transform_conversation(example):\n    # Assuming the first column contains the context and the second column contains the response\n    context = example['Context']\n    response = example['Response']\n\n    # Apply any transformation you need here\n    transformed_example = {'text': f'<s>[CONTEXT] {context} [/CONTEXT] {response} </s>'}\n\n    return transformed_example\n\n# Apply the transformation to the subset\ntransformed_second_dataset = second_dataset.map(transform_conversation)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-16T16:54:23.540615Z","iopub.execute_input":"2024-04-16T16:54:23.541013Z","iopub.status.idle":"2024-04-16T16:54:24.832770Z","shell.execute_reply.started":"2024-04-16T16:54:23.540973Z","shell.execute_reply":"2024-04-16T16:54:24.831832Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3512 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"033f27cbe0ce4d7e95dd6b24a156bf95"}},"metadata":{}}]},{"cell_type":"code","source":"transformed_second_dataset.push_to_hub(\"SiddharthShukla48/Mental_Health_Final\")","metadata":{"execution":{"iopub.status.busy":"2024-04-16T16:55:14.048805Z","iopub.execute_input":"2024-04-16T16:55:14.049199Z","iopub.status.idle":"2024-04-16T16:55:15.972318Z","shell.execute_reply.started":"2024-04-16T16:55:14.049171Z","shell.execute_reply":"2024-04-16T16:55:15.971389Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf5c532c46514f0ab84aa5a7a575dc03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating parquet from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7aba1168e57949a7abad53ddd130c256"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/datasets/SiddharthShukla48/Mental_Health_Final/commit/80351b9ecf97a12f48555058fd862dffa5c36254', commit_message='Upload dataset', commit_description='', oid='80351b9ecf97a12f48555058fd862dffa5c36254', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.33.1 trl==0.4.7","metadata":{"execution":{"iopub.status.busy":"2024-04-16T16:55:16.382790Z","iopub.execute_input":"2024-04-16T16:55:16.383131Z","iopub.status.idle":"2024-04-16T16:55:47.396862Z","shell.execute_reply.started":"2024-04-16T16:55:16.383105Z","shell.execute_reply":"2024-04-16T16:55:47.395590Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2024-04-16T16:55:47.398695Z","iopub.execute_input":"2024-04-16T16:55:47.399023Z","iopub.status.idle":"2024-04-16T16:56:19.056078Z","shell.execute_reply.started":"2024-04-16T16:55:47.398995Z","shell.execute_reply":"2024-04-16T16:56:19.055272Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"2024-04-16 16:56:00.241831: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-16 16:56:00.241961: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-16 16:56:00.497811: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# The model that you want to train from the Hugging Face hub\nmodel_name = \"NousResearch/Llama-2-7b-chat-hf\"\n\n# The instruction dataset to use\ndataset_name = \"SiddharthShukla48/Mental_Health_Final\"\n\n# Fine-tuned model name\nnew_model = \"Llama-2-7b-chat-MH\"\n\n################################################################################\n# QLoRA parameters\n################################################################################\n\n# LoRA attention dimension\nlora_r = 64\n\n# Alpha parameter for LoRA scaling\nlora_alpha = 16\n\n# Dropout probability for LoRA layers\nlora_dropout = 0.1\n\n################################################################################\n# bitsandbytes parameters\n################################################################################\n\n# Activate 4-bit precision base model loading\nuse_4bit = True\n\n# Compute dtype for 4-bit base models\nbnb_4bit_compute_dtype = \"float16\"\n\n# Quantization type (fp4 or nf4)\nbnb_4bit_quant_type = \"nf4\"\n\n# Activate nested quantization for 4-bit base models (double quantization)\nuse_nested_quant = False\n\n################################################################################\n# TrainingArguments parameters\n################################################################################\n\n# # Output directory where the model predictions and checkpoints will be stored\noutput_dir = \"SiddharthShukla48/llama_mental_health\"\n\n# Number of training epochs\nnum_train_epochs = 1\n\n# Enable fp16/bf16 training (set bf16 to True with an A100)\nfp16 = False\nbf16 = False\n\n# Batch size per GPU for training\nper_device_train_batch_size = 4\n\n# Batch size per GPU for evaluation\nper_device_eval_batch_size = 4\n\n# Number of update steps to accumulate the gradients for\ngradient_accumulation_steps = 1\n\n# Enable gradient checkpointing\ngradient_checkpointing = True\n\n# Maximum gradient normal (gradient clipping)\nmax_grad_norm = 0.3\n\n# Initial learning rate (AdamW optimizer)\nlearning_rate = 2e-4\n\n# Weight decay to apply to all layers except bias/LayerNorm weights\nweight_decay = 0.001\n\n# Optimizer to use\noptim = \"paged_adamw_32bit\"\n\n# Learning rate schedule\nlr_scheduler_type = \"cosine\"\n\n# Number of training steps (overrides num_train_epochs)\nmax_steps = -1\n\n# Ratio of steps for a linear warmup (from 0 to learning rate)\nwarmup_ratio = 0.03\n\n# Group sequences into batches with same length\n# Saves memory and speeds up training considerably\ngroup_by_length = True\n\n# Save checkpoint every X updates steps\nsave_steps = 0\n\n# Log every X updates steps\nlogging_steps = 25\n\n################################################################################\n# SFT parameters\n################################################################################\n\n# Maximum sequence length to use\nmax_seq_length = None\n\n# Pack multiple short examples in the same input sequence to increase efficiency\npacking = False\n\n# Load the entire model on the GPU 0\ndevice_map = {\"\": 0}","metadata":{"execution":{"iopub.status.busy":"2024-04-16T16:56:19.057884Z","iopub.execute_input":"2024-04-16T16:56:19.058197Z","iopub.status.idle":"2024-04-16T16:56:19.068135Z","shell.execute_reply.started":"2024-04-16T16:56:19.058172Z","shell.execute_reply":"2024-04-16T16:56:19.067135Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Load dataset (you can process it here)\ndataset = load_dataset(dataset_name, split=\"train\")\n\n# Load tokenizer and model with QLoRA configuration\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\n\n# Check GPU compatibility with bfloat16\nif compute_dtype == torch.float16 and use_4bit:\n    major, _ = torch.cuda.get_device_capability()\n    if major >= 8:\n        print(\"=\" * 80)\n        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n        print(\"=\" * 80)\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=device_map\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n\n# Load LoRA configuration\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\n# Set training parameters\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to=\"tensorboard\"\n)\n\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n)\n\n# Train model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-16T16:56:19.069161Z","iopub.execute_input":"2024-04-16T16:56:19.069395Z","iopub.status.idle":"2024-04-16T18:09:57.617008Z","shell.execute_reply.started":"2024-04-16T16:56:19.069374Z","shell.execute_reply":"2024-04-16T18:09:57.616108Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/347 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd3f02e2123841779e2cdeb289378a3b"}},"metadata":{}},{"name":"stderr","text":"Downloading data: 100%|██████████| 5.02M/5.02M [00:00<00:00, 15.3MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/3512 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3850fc490611469297466dc6309a5843"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"147efae917144abe8aac6ec4358774aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f8fb9358c484e1e9812b3b2e10c07fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"168a66921add4b0ebc9e45db699095ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b73f994f3d93455ba0fb7c08037ac4b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5527ef81c1445b09b1ed15cd05c4ca2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da3c4e34889046b184d461632ec15dfb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/179 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94277392510f4baf83fcd5ca78d9eafb"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fbc27507d9d4a2a86395798044daa28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e892cf2df46e4145a70fdcb5c71c88ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99000690bba34d13956163f6bbe0c36a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a6716002aa54509ac9cc940c309d01f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42ebcd90cc74449d8c6ecaa6b2a65b9a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3512 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d728cf5a9be84c78b9137392102dab3d"}},"metadata":{}},{"name":"stderr","text":"You are using 8-bit optimizers with a version of `bitsandbytes` < 0.41.1. It is recommended to update your version as a major bug has been fixed in 8-bit optimizers.\nYou're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='439' max='439' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [439/439 1:11:24, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>2.415500</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>2.177800</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>2.150400</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.103600</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>2.049300</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>2.069200</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>2.074400</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>2.043000</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>2.057200</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>2.008700</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>2.033200</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.973500</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>2.054900</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.964100</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>2.006900</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>2.014500</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>2.010100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=439, training_loss=2.067362631100457, metrics={'train_runtime': 4315.4176, 'train_samples_per_second': 0.814, 'train_steps_per_second': 0.102, 'total_flos': 2.349778193252352e+16, 'train_loss': 2.067362631100457, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"# Save trained model\ntrainer.push_to_hub(\"SiddharthShukla48/llama_mental_health\")","metadata":{"execution":{"iopub.status.busy":"2024-04-16T18:09:57.618758Z","iopub.execute_input":"2024-04-16T18:09:57.619076Z","iopub.status.idle":"2024-04-16T18:10:03.863378Z","shell.execute_reply.started":"2024-04-16T18:09:57.619051Z","shell.execute_reply":"2024-04-16T18:10:03.862493Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16854c05ba064d0db1b69ed16b65131f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/4.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afc11eb098fd408a8d0574daf1ec6903"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.bin:   0%|          | 0.00/134M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a3899d23b5a4bd78e8a3557fabc26df"}},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/SiddharthShukla48/llama_mental_health/commit/be5d1449487c6c4790f5c54a95ee4b6d4215b925', commit_message='SiddharthShukla48/llama_mental_health', commit_description='', oid='be5d1449487c6c4790f5c54a95ee4b6d4215b925', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"# Ignore warnings\nimport logging\nlogging.getLogger().setLevel(logging.CRITICAL)\n\n# Run text generation pipeline with our next model\nprompt = \"I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here. How can I change my feeling of being worthless to everyone?\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n\n# Generate text\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\n\n# Print entire result object\nprint(result)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-16T18:17:30.988274Z","iopub.execute_input":"2024-04-16T18:17:30.988998Z","iopub.status.idle":"2024-04-16T18:18:14.679196Z","shell.execute_reply.started":"2024-04-16T18:17:30.988953Z","shell.execute_reply":"2024-04-16T18:18:14.678183Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"[{'generated_text': \"<s>[INST] I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here. How can I change my feeling of being worthless to everyone? [/INST] Feeling worthless is a very common feeling that many people experience. It is important to remember that you are not alone in feeling this way. It is also important to remember that you are not worthless. You are a human being with many wonderful qualities and attributes. You are a unique and special person. You are loved and appreciated by many people. You are a valuable member of your family and community. You are a person with many talents and abilities. You are a person with many strengths and accomplishments. You are a person with many positive qualities. You are a person with many wonderful attributes. You are a person with many wonderful qualities. You are a person\"}]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}